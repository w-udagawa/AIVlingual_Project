# CLAUDE.md - AIVlingual Development Guide

## Project Overview
AIVlingual is a revolutionary YouTube channel system that transforms ephemeral Vtuber content into permanent educational resources. The core innovation is converting "flow content" (Vtuber clips) into "stock content" (searchable language learning database).

### Key Achievements
- **Fully Functional Bilingual AI Vtuber Assistant** - AI character "Rin (ã‚Šã‚“)" responds naturally in Japanese/English based on user input with proper language detection and mixing (70/30 rule)
- **Complete User Authentication System** - JWT-based login/registration with user preferences
- **Settings Management** - User can customize language, AI response, and export preferences
- **Comprehensive E2E Testing** - Playwright-based testing framework with complete video analysis workflow testing
- **NLP-Enhanced Vocabulary Extraction** - spaCy integration with CEFR datasets for unlimited vocabulary extraction

## Development Commands

### Quick Start
```bash
# Backend (Windowsã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å®Ÿè¡Œ)
cd backend
conda activate aivlingual_py311
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Frontend (åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«)
cd frontend
npm run dev    # Frontend on port 3003
```

**é‡è¦**: 
- NLPã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€å¿…ãšWindowsã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å®Ÿè¡Œï¼ˆWSL2ã§ã¯NLPãŒå‹•ä½œã—ã¾ã›ã‚“ï¼‰
- `python -m uvicorn`å½¢å¼ã§èµ·å‹•ã™ã‚‹ã“ã¨ã§ã€æ­£ã—ã„Condaç’°å¢ƒãŒä½¿ç”¨ã•ã‚Œã¾ã™

### Backend Commands
```bash
cd backend

# Environment Setup (Conda recommended)
conda create -n aivlingual_py311 python=3.11
conda activate aivlingual_py311
pip install -r requirements.txt
pip install -r requirements-nlp.txt
python -m spacy download en_core_web_sm
python -m spacy download ja_core_news_sm

# Development
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
python main.py  # Alternative with proper shutdown handling

# Testing
pytest                              # Run all tests
pytest -v                          # Verbose output
pytest tests/test_vocabulary_extractor.py::test_extract_expressions -v  # Single test
pytest tests/test_batch_processing.py
pytest tests/test_ai_responder.py
pytest tests/test_speech_processor.py

# Code Quality
black app/                         # Format code
mypy app/                         # Type checking
ruff check app/                   # Linting
```

### Frontend Commands
```bash
cd frontend

# Development
npm install                       # Install dependencies
npm run dev                      # Start dev server (port 3003)
npm run build                    # Build for production
npm run preview                  # Preview production build

# Testing
npm test                         # Run unit tests
npm run test:watch              # Watch mode
npm run test:coverage           # Coverage report

# E2E Testing (Playwright)
npm run test:e2e                # Run all E2E tests
npm run test:e2e:ui            # Run with UI mode
npm run test:e2e:headed        # Run with browser visible
npm run test:e2e:debug         # Debug mode
npx playwright test auth.spec.ts  # Run specific test
npx playwright test video-analysis.spec.ts --project=chromium

# Code Quality
npm run lint                    # Lint code
npm run type-check             # TypeScript checking
```

### Database Commands
```bash
# Run migrations
cd backend
python -m app.migrations.migration_runner

# Test database connection
python -c "from app.services.database_service import db_service; import asyncio; asyncio.run(db_service.init_db())"
```

## Architecture Overview

### Flow-to-Stock Pipeline
```
Vtuber Clip â†’ Transcript â†’ AI Analysis â†’ Notion Database â†’ Multi-format Export
                              â†“
                    SEO-optimized content â†’ YouTube/Social Media
```

### Technology Stack
- **AI**: Gemini 2.0 Flash (ultra-low latency, streaming support)
- **Backend**: Python 3.11+, FastAPI, WebSocket, SQLite
- **Frontend**: React, TypeScript, Vite, Tailwind CSS
- **Speech**: Web Speech API (browser-based, free)
- **Database**: SQLite (local) + Notion API (public)
- **Testing**: Pytest (backend), Jest + Playwright (frontend)
- **NLP**: spaCy + GiNZA (Japanese), CEFR datasets

### Project Structure
```
AIVlingual_Project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/              # API endpoints & WebSocket handlers
â”‚   â”‚   â”‚   â””â”€â”€ v1/endpoints/
â”‚   â”‚   â”‚       â”œâ”€â”€ auth.py   # Authentication endpoints
â”‚   â”‚   â”‚       â”œâ”€â”€ vocabulary.py
â”‚   â”‚   â”‚       â””â”€â”€ youtube.py
â”‚   â”‚   â”œâ”€â”€ core/             # Business logic, AI integration
â”‚   â”‚   â”œâ”€â”€ services/         # External service integrations
â”‚   â”‚   â”‚   â”œâ”€â”€ auth_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ database_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ export_service.py
â”‚   â”‚   â”‚   â””â”€â”€ nlp_vocabulary_extractor.py  # NLP-enhanced extraction
â”‚   â”‚   â”œâ”€â”€ models/           # Data models
â”‚   â”‚   â”‚   â””â”€â”€ user.py       # User authentication models
â”‚   â”‚   â””â”€â”€ migrations/       # Database migrations
â”‚   â”œâ”€â”€ scripts/              # Organized utility scripts
â”‚   â”‚   â””â”€â”€ nlp_setup/       # NLP setup batch files
â”‚   â”œâ”€â”€ tests/               # Backend unit tests
â”‚   â”œâ”€â”€ logs/                # Application log files
â”‚   â””â”€â”€ test_outputs/        # Test export files
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/       # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ ProtectedRoute.tsx
â”‚   â”‚   â”‚   â””â”€â”€ UserMenu.tsx
â”‚   â”‚   â”œâ”€â”€ contexts/         # React contexts
â”‚   â”‚   â”‚   â””â”€â”€ AuthContext.tsx
â”‚   â”‚   â”œâ”€â”€ pages/           # Page components
â”‚   â”‚   â”‚   â”œâ”€â”€ LoginPage.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ MainPage.tsx
â”‚   â”‚   â”‚   â””â”€â”€ SettingsPage.tsx
â”‚   â”‚   â”œâ”€â”€ services/         # API clients
â”‚   â”‚   â”‚   â””â”€â”€ authService.ts
â”‚   â”‚   â””â”€â”€ obs/             # OBS-specific features
â”‚   â”œâ”€â”€ e2e/                  # Playwright E2E tests
â”‚   â”‚   â”œâ”€â”€ fixtures/         # Test fixtures
â”‚   â”‚   â”œâ”€â”€ page-objects/     # Page Object Models
â”‚   â”‚   â””â”€â”€ tests/           # Test specs
â”‚   â””â”€â”€ public/
â”œâ”€â”€ docs/                     # Documentation
â”œâ”€â”€ scripts/                  # Setup and startup scripts
â””â”€â”€ tests/                    # Integration tests
```

## Critical Environment Variables

```bash
# backend/.env
GEMINI_API_KEY=your_key_here              # Required for AI responses
YOUTUBE_API_KEY=your_key_here            # For video analysis
NOTION_TOKEN=ntn_xxxx                    # For database sync
NOTION_DATABASE_ID=32_char_hex_no_dashes # e.g., 22104becf05280528c28c1a0505947ef

# Optional
STREAM_ENABLED=false                     # Streaming responses (disabled for stability)
AI_TEMPERATURE=0.7                       # AI creativity level
DATABASE_URL=sqlite:///./aivlingual.db   # Database location
JWT_SECRET_KEY=your_secret_key          # For authentication
```

## Common Development Tasks

### Running a Single Test
```bash
# Backend
pytest tests/test_vocabulary_extractor.py::test_extract_expressions -v

# Frontend E2E
npx playwright test video-analysis.spec.ts --project=chromium
```

### Testing Video Analysis
```bash
# Single video
curl "http://localhost:8000/api/v1/youtube/extract-vocabulary?url=https://youtu.be/VIDEO_ID"

# Batch processing
curl -X POST "http://localhost:8000/api/v1/youtube/batch-extract" \
  -H "Content-Type: application/json" \
  -d '{"urls": ["https://youtu.be/VIDEO1", "https://youtu.be/VIDEO2"]}'
```

### WebSocket Testing
```javascript
// Browser console
const ws = new WebSocket('ws://localhost:8000/ws/audio');
ws.onopen = () => ws.send(JSON.stringify({type: 'web_speech_result', text: 'Hello'}));
ws.onmessage = (e) => console.log('Response:', JSON.parse(e.data));
```

### NLP Extraction Testing
```bash
# Test NLP-enhanced extraction
cd tests
python test_nlp_extraction.py
```

## High-Level Architecture

### AI Response Pipeline
1. **User Input** â†’ Web Speech API (browser) â†’ WebSocket â†’ Backend
2. **Language Detection** â†’ Detects Japanese/English/Mixed with confidence scores
3. **AI Processing** â†’ Gemini 2.0 Flash with character prompt (Rin/ã‚Šã‚“)
4. **Response Generation** â†’ 70/30 language mixing based on input language
5. **TTS** â†’ Browser speech synthesis with Japanese voice selection
6. **WebSocket Response** â†’ Frontend display with typing animation

### Vocabulary Extraction Pipeline
1. **YouTube URL** â†’ youtube-transcript-api â†’ Extract captions
2. **NLP Processing** â†’ spaCy extracts phrasal verbs, collocations, idioms
3. **Pattern Matching** â†’ Detect Vtuber slang, expressions, grammar
4. **CEFR Level Assignment** â†’ Automatic difficulty using Oxford 3000/5000
5. **AI Enhancement** â†’ Gemini analyzes context and educational value
6. **Database Storage** â†’ SQLite cache + Notion public database
7. **Export Options** â†’ CSV, JSON, Anki deck formats

### WebSocket Message Flow
```javascript
// Client â†’ Server
{
  type: 'web_speech_result',
  text: 'ä»Šæ—¥ã¯ä½•ã‚’å‹‰å¼·ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ',
  language: 'ja-JP',
  isFinal: true
}

// Server â†’ Client
{
  type: 'ai_response',
  text: 'ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã¯äººæ°—Vtuberã®ã‚¯ãƒªãƒƒãƒ—ã‹ã‚‰...',
  language: 'ja',
  isComplete: true
}
```

### Authentication API
```javascript
// Login Request (æ³¨æ„: username_or_email ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨)
POST /api/v1/auth/login
{
  username_or_email: "test",  // username ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã¯ãªã„ï¼
  password: "test0702"
}

// Login Response
{
  access_token: "eyJ...",     // token ã§ã¯ãªã access_token
  token_type: "bearer",
  user: {
    id: 1,
    username: "test",
    email: "test@example.com",
    is_active: true,
    is_verified: false
  }
}

// Register Request
POST /api/v1/auth/register
{
  username: "test",
  email: "test@example.com",
  password: "test0702"
}
```

## Key Implementation Details

### Language Detection Logic
- Japanese detection: >30% Japanese characters â†’ Japanese-primary response
- Mixed content: Significant presence of both languages
- Response mixing: 70% primary language, 30% secondary language

### NLP Extraction Features
- **spaCy Integration**: Phrasal verbs, collocations, named entities
- **CEFR Datasets**: Automatic difficulty assignment (A1-C2)
- **Redis Caching**: 24-hour TTL for processed videos
- **Hybrid Approach**: Pattern matching + NLP + AI enhancement

### Rate Limiting
- Gemini API: 60 requests/minute per client
- YouTube API: 10,000 units/day quota
- WebSocket: 1 message/second per client

### Error Recovery
- AI response timeout: 30 seconds with automatic reset
- WebSocket reconnection: Exponential backoff
- TTS fallback: Multiple voice options with quality preference

### Database Schema
- **vocabulary** table: Stores extracted expressions with metadata
- **vocabulary_cache**: YouTube video analysis results  
- **users**: Authentication and preferences
- **user_vocabulary**: User-specific vocabulary tracking
- **user_settings**: Language preferences and export options

## Testing Guidelines

### E2E Test Structure
```
frontend/e2e/
â”œâ”€â”€ fixtures/test-base.ts      # Custom fixtures & auth helpers
â”œâ”€â”€ page-objects/              # Page Object Models
â”‚   â”œâ”€â”€ HomePage.ts
â”‚   â”œâ”€â”€ VideoAnalysisPage.ts
â”‚   â””â”€â”€ VocabularyPage.ts
â””â”€â”€ tests/                     # Test specifications
    â”œâ”€â”€ auth.spec.ts          # Authentication flows
    â”œâ”€â”€ video-analysis.spec.ts # YouTube analysis
    â””â”€â”€ websocket.spec.ts     # Real-time features
```

### Running E2E Tests
```bash
# Ensure backend and frontend are running
cd backend && python main.py &
cd frontend && npm run dev &

# Run tests
cd frontend
npm run test:e2e              # Headless mode
npm run test:e2e:ui          # Interactive UI
npx playwright test --debug   # Debug specific test
```

## Troubleshooting

### Common Issues
1. **"AIãŒè€ƒãˆã¦ã„ã¾ã™..." stuck**: Check GEMINI_API_KEY and WebSocket connection
2. **Japanese TTS issues**: Verify browser has Japanese voices installed
3. **WebSocket errors**: Check CORS settings include port 3003
4. **Database errors**: Run migrations with `python -m app.migrations.migration_runner`
5. **E2E test failures**: Ensure test data is clean, run with `--headed` to debug
6. **Login returns 422 error**: 
   - Ensure using `username_or_email` field (not `username`) in login request
   - Check response expects `access_token` (not `token`)
7. **React-hot-toast test failures**: Use `[role="status"]` selector with filter for specific text
8. **API endpoint `/api/v1/youtube/analyze` not found**: 
   - Use `/api/v1/youtube/extract-vocabulary` instead
   - Update API config mapping in frontend services
9. **Data field mapping errors in video analysis**:
   - API returns `japanese_text`/`english_text`, frontend expects `japanese`/`english`
   - API returns `channel_title`/`thumbnail_url`, frontend expects `channel`/`thumbnail`
   - Implement data transformation in YouTubeService
10. **React component errors**: `Cannot read properties of undefined (reading 'length')`
    - Add optional chaining: `vocabulary_items?.length`
    - Check for undefined before accessing nested properties
11. **authenticatedPage fixture not found in E2E tests**:
    - Replace with loginPage fixture and manual login flow
    - Update test beforeEach hooks to handle authentication
12. **NLP not working (only 3 vocabulary items extracted)**:
    - **æœ€é‡è¦**: Windowsã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å®Ÿè¡Œã—ã¦ã„ã‚‹ã‹ç¢ºèªï¼ˆWSL2ã§ã¯å‹•ä½œã—ã¾ã›ã‚“ï¼‰
    - æ­£ã—ã„èµ·å‹•æ–¹æ³•: `python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000`
    - `/api/v1/youtube/debug-env`ã§ç’°å¢ƒã‚’ç¢ºèª
    - è©³ç´°ã¯ `/docs/NLP_TROUBLESHOOTING_GUIDE.md` ã‚’å‚ç…§

### Debug Commands
```bash
# Check API keys
python -c "from app.core.config import settings; print(f'Gemini: {settings.GEMINI_API_KEY[:10]}...')"

# Test WebSocket
curl -i -N -H "Connection: Upgrade" -H "Upgrade: websocket" http://localhost:8000/ws/audio

# View logs
tail -f backend/logs/backend.log
tail -f frontend/logs/frontend.log

# Test NLP functionality
cd backend
conda activate aivlingual_py311
python -c "import spacy; nlp = spacy.load('ja_core_news_sm'); print('Japanese NLP ready')"
```

## Recent Major Changes

### 2025-01-10
- **NLP Extraction Fix**: Resolved issue where only 3-5 vocabulary items were extracted due to WSL2/Conda environment mismatch
- **Vocabulary Quality Improvements**: Enhanced filtering to exclude A1 level words, improved collocation quality, added VTuber/gaming expressions
- **Troubleshooting Guide**: Created comprehensive NLP troubleshooting documentation
- **Startup Method Update**: Clarified that backend must run in Windows Command Prompt with Conda environment

### 2025-07-05
- **NLP Environment Setup**: Configured Miniconda with Python 3.11 for spaCy compatibility
- **Japanese Model Integration**: Integrated ja_core_news_sm for Japanese NLP processing
- **Project Reorganization**: Cleaned up scripts, moved to organized directory structure
- **Improved Error Handling**: Fixed pydantic/spaCy version conflicts

### 2025-07-03
- **NLP-Enhanced Vocabulary Extraction**: Implemented spaCy integration with CEFR datasets for unlimited vocabulary extraction
- **Hybrid Extraction Architecture**: 3-layer system combining spaCy NLP, CEFR datasets, and Gemini AI
- **Intelligent Caching**: Redis-based caching with 24-hour TTL and memory fallback
- **Multi-word Expression Detection**: Automatic detection of phrasal verbs, idioms, and collocations
- **CEFR Level Assignment**: Automatic difficulty assignment using Oxford 3000/5000 datasets

### 2025-07-02
- **Comprehensive E2E Testing Framework**: Implemented Playwright testing with fixtures, Page Object Models, and complete video analysis workflow testing
- **API Endpoint Corrections**: Fixed missing `/api/v1/youtube/analyze` endpoint mapping to `/extract-vocabulary`
- **Data Mapping Resolution**: Resolved API response field mismatches between backend and frontend
- **Component Error Handling**: Added optional chaining and proper null checks for React components
- **Authentication System Fixes**: Resolved login credential validation (6-char minimum password) and JWT token handling
- **CORS Enhancement**: Updated configuration to support multiple frontend ports (3003, 3004)
- **User Authentication System**: Added JWT-based login/registration with user preferences
- **Multi-format Export**: Created CSV, JSON, and Anki deck export functionality
- **WebSocket Optimization**: Fixed message handling and language detection issues
- **TTS Enhancement**: Proper Japanese voice selection and pronunciation
- **Settings Management**: User preferences and form validation with onBlur events

### Architecture Decisions
- **Gemini 2.0 Flash**: Chosen for <1s latency and free tier
- **Web Speech API**: Zero-cost browser-based speech recognition
- **Vite + React**: Fast development with HMR
- **SQLite + Notion**: Local performance + public SEO benefits
- **Playwright**: Cross-browser E2E testing with visual debugging
- **spaCy + GiNZA**: Advanced NLP for unlimited vocabulary extraction
- **CEFR Datasets**: Standardized difficulty levels for language learners

---

**Remember**: This project transforms entertainment into education. Every Vtuber clip processed creates permanent learning value! ğŸš€