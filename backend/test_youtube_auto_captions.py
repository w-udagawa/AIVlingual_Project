#!/usr/bin/env python
"""
YouTubeè‡ªå‹•ç”Ÿæˆå­—å¹•ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ—¥æœ¬ã®VTuberå‹•ç”»ã§è‡ªå‹•ç”Ÿæˆå­—å¹•ã®å–å¾—ã‚’ãƒ†ã‚¹ãƒˆ
"""
import asyncio
import sys
sys.path.append('.')

from app.services.youtube_service import YouTubeService
from app.services.nlp_vocabulary_extractor import nlp_extractor

# æ—¥æœ¬ã®VTuberå‹•ç”»ï¼ˆè‡ªå‹•ç”Ÿæˆå­—å¹•ãŒæœŸå¾…ã§ãã‚‹ã‚‚ã®ï¼‰
TEST_VIDEOS = {
    # ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–
    "5nQvHirtfBg": "ãºã“ã‚‰ - ãƒžã‚¤ã‚¯ãƒ©é…ä¿¡",
    "bKjmUvilMLE": "ã¿ã“ - é›‘è«‡é…ä¿¡",
    "9s_TPebdIRU": "ã‚¹ãƒãƒ« - ã‚²ãƒ¼ãƒ å®Ÿæ³",
    
    # ã«ã˜ã•ã‚“ã˜
    "z5OGD5_9cA4": "è‘›è‘‰ - Apexé…ä¿¡",
    "x1lGOgPkctc": "å¶ - é›‘è«‡",
    
    # ãã®ä»–äººæ°—Vtuber
    "Lu5VCO-YdqI": "ã“ã‚ã­ - è‹±èªžå‹‰å¼·",
    "HKYkhkYGG7A": "ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›å‹•ç”»",
}

async def test_auto_captions():
    """è‡ªå‹•ç”Ÿæˆå­—å¹•ã®ãƒ†ã‚¹ãƒˆ"""
    youtube_service = YouTubeService()
    successful_videos = []
    
    print("YouTubeè‡ªå‹•ç”Ÿæˆå­—å¹•ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    for video_id, description in TEST_VIDEOS.items():
        print(f"\nå‹•ç”»: {video_id} ({description})")
        print("-" * 40)
        
        try:
            # è‡ªå‹•ç”Ÿæˆå­—å¹•ã‚’å„ªå…ˆã—ã¦å–å¾—
            print("1. è‡ªå‹•ç”Ÿæˆå­—å¹•ã‚’å„ªå…ˆã—ã¦å–å¾—...")
            transcript = await youtube_service.get_transcript(
                video_id, 
                languages=['ja', 'en'], 
                prefer_auto_generated=True
            )
            
            if transcript:
                print(f"âœ… å­—å¹•å–å¾—æˆåŠŸï¼ {len(transcript)}å€‹ã®ã‚¨ãƒ³ãƒˆãƒª")
                
                # æœ€åˆã®5ã¤ã‚’è¡¨ç¤º
                print("\nå­—å¹•ã‚µãƒ³ãƒ—ãƒ«:")
                for i, entry in enumerate(transcript[:5]):
                    print(f"  [{entry['start']:.1f}s] {entry['text']}")
                
                # å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
                full_text = " ".join([entry['text'] for entry in transcript])
                print(f"\nç·æ–‡å­—æ•°: {len(full_text)}æ–‡å­—")
                
                # NLPèªžå½™æŠ½å‡º
                print("\n2. NLPèªžå½™æŠ½å‡º...")
                vocabulary = await nlp_extractor.extract_from_text_nlp(full_text[:3000], 'japanese')  # æœ€åˆã®3000æ–‡å­—
                print(f"âœ… {len(vocabulary)}å€‹ã®èªžå½™ã‚’æŠ½å‡º")
                
                if len(vocabulary) >= 30:
                    print("ðŸŽ‰ æˆåŠŸï¼30å€‹ä»¥ä¸Šã®èªžå½™ã‚’æŠ½å‡ºã§ãã¾ã—ãŸï¼")
                    successful_videos.append({
                        'video_id': video_id,
                        'description': description,
                        'transcript_entries': len(transcript),
                        'vocabulary_count': len(vocabulary)
                    })
                else:
                    print(f"âš ï¸ èªžå½™æ•°ãŒå°‘ãªã„ï¼ˆ{len(vocabulary)}å€‹ï¼‰")
                
                # æŠ½å‡ºã•ã‚ŒãŸèªžå½™ã®ä¾‹
                print("\næŠ½å‡ºã•ã‚ŒãŸèªžå½™ï¼ˆä¸Šä½5å€‹ï¼‰:")
                for i, vocab in enumerate(vocabulary[:5]):
                    print(f"  {i+1}. {vocab['expression']} (é›£æ˜“åº¦: {vocab.get('difficulty', '?')})")
                
            else:
                print("âŒ å­—å¹•å–å¾—å¤±æ•—")
                
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # çµæžœã‚µãƒžãƒªãƒ¼
    print("\n" + "=" * 60)
    print("ãƒ†ã‚¹ãƒˆçµæžœã‚µãƒžãƒªãƒ¼")
    print("=" * 60)
    
    print(f"\nãƒ†ã‚¹ãƒˆå‹•ç”»æ•°: {len(TEST_VIDEOS)}")
    print(f"æˆåŠŸ: {len(successful_videos)}å€‹")
    
    if successful_videos:
        print("\nâœ… æˆåŠŸã—ãŸå‹•ç”»:")
        for video in successful_videos:
            print(f"  - {video['video_id']}: {video['description']}")
            print(f"    å­—å¹•: {video['transcript_entries']}ã‚¨ãƒ³ãƒˆãƒª, èªžå½™: {video['vocabulary_count']}å€‹")
        
        print("\næŽ¨å¥¨: ã“ã‚Œã‚‰ã®å‹•ç”»IDã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼")
        print("å‹•ç”»ID:", ", ".join([v['video_id'] for v in successful_videos]))
    else:
        print("\nâŒ ã™ã¹ã¦ã®å‹•ç”»ã§å¤±æ•—ã—ã¾ã—ãŸã€‚")
        print("è€ƒãˆã‚‰ã‚Œã‚‹åŽŸå› :")
        print("- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯/ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ã®å•é¡Œ")
        print("- YouTube APIã®åˆ¶é™")
        print("- å‹•ç”»ã®åœ°åŸŸåˆ¶é™")

async def test_specific_video(video_id: str):
    """ç‰¹å®šã®å‹•ç”»ã‚’è©³ç´°ã«ãƒ†ã‚¹ãƒˆ"""
    youtube_service = YouTubeService()
    
    print(f"\nç‰¹å®šå‹•ç”»ã®è©³ç´°ãƒ†ã‚¹ãƒˆ: {video_id}")
    print("=" * 60)
    
    # å‹•ç”»æƒ…å ±ã‚’å–å¾—
    video_info = await youtube_service.get_video_info(video_id)
    if video_info:
        print(f"ã‚¿ã‚¤ãƒˆãƒ«: {video_info.get('title', 'N/A')}")
        print(f"ãƒãƒ£ãƒ³ãƒãƒ«: {video_info.get('channel_title', 'N/A')}")
        print(f"åˆ©ç”¨å¯èƒ½ãªå­—å¹•: {len(video_info.get('available_transcripts', []))}ç¨®é¡ž")
        
        for transcript in video_info.get('available_transcripts', []):
            print(f"  - {transcript['language_code']} ({transcript['language']}) {'[è‡ªå‹•ç”Ÿæˆ]' if transcript['is_generated'] else '[æ‰‹å‹•]'}")
    
    # é€šå¸¸ã®æ–¹æ³•ã§å–å¾—
    print("\n1. é€šå¸¸ã®å–å¾—æ–¹æ³•:")
    normal_transcript = await youtube_service.get_transcript(video_id)
    if normal_transcript:
        print(f"âœ… æˆåŠŸ: {len(normal_transcript)}ã‚¨ãƒ³ãƒˆãƒª")
    else:
        print("âŒ å¤±æ•—")
    
    # è‡ªå‹•ç”Ÿæˆå„ªå…ˆã§å–å¾—
    print("\n2. è‡ªå‹•ç”Ÿæˆå„ªå…ˆ:")
    auto_transcript = await youtube_service.get_transcript(video_id, prefer_auto_generated=True)
    if auto_transcript:
        print(f"âœ… æˆåŠŸ: {len(auto_transcript)}ã‚¨ãƒ³ãƒˆãƒª")
    else:
        print("âŒ å¤±æ•—")

if __name__ == "__main__":
    print("YouTubeè‡ªå‹•ç”Ÿæˆå­—å¹•ã¨NLPèªžå½™æŠ½å‡ºãƒ†ã‚¹ãƒˆ\n")
    
    # ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ
    asyncio.run(test_auto_captions())
    
    # ç‰¹å®šã®å‹•ç”»ã‚’è©³ç´°ãƒ†ã‚¹ãƒˆ
    if len(sys.argv) > 1:
        video_id = sys.argv[1]
        asyncio.run(test_specific_video(video_id))